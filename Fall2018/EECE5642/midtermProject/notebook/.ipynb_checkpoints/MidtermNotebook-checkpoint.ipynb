{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.1'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sklearn inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import nltk as nl\n",
    "import sklearn\n",
    "#import gensim \n",
    "\n",
    "sklearn.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the data set to be trained \n",
    "training_set = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from the text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(training_set.data)\n",
    "#X_train_counts.shape\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess \n",
    "# Remove stop words and punctuation\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=remove)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=remove)\n",
    "\n",
    "# only keep letters & make them all lower case\n",
    "news = [' '.join(filter(str.isalpha, raw.lower().split())) for raw in\n",
    "        newsgroups_train.data + newsgroups_test.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing something\n",
      "something else\n",
      "where are we\n",
      "[[0.00238095 0.00238095 0.00238095 ... 0.00238095 0.00238095 0.00238095]\n",
      " [0.00138889 0.06929068 0.00138889 ... 0.00138889 0.00138889 0.09176861]\n",
      " [0.00048544 0.00048544 0.00048544 ... 0.00048544 0.00048544 0.00048544]\n",
      " ...\n",
      " [0.03973621 0.00142857 0.15997102 ... 0.00142857 0.00142857 0.00142857]\n",
      " [0.02234043 0.00106383 0.00106383 ... 0.00106383 0.00106383 0.00106383]\n",
      " [0.00263158 0.00263158 0.45299728 ... 0.00263158 0.00263158 0.00263158]]\n"
     ]
    }
   ],
   "source": [
    "#LDA , tf-idf\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "num_topics = 20\n",
    "num_iterations = 5\n",
    "print(\"doing something\")\n",
    "# need to ignore the stop words - stop_words=\"english\" \n",
    "#ignore words that appear less than 5 times\n",
    "vectorized = CountVectorizer(min_df=5, stop_words='english')\n",
    "vectorized_fit = vectorized.fit_transform(news)\n",
    "print('something else')\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, max_iter=num_iterations, learning_method='online',learning_offset=50.,random_state=0).fit(vectorized_fit)\n",
    "print('where are we')\n",
    "X_topics = lda.fit_transform(vectorized_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-77927dcb8150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0malldocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_docs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malldocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'size' is not defined"
     ]
    }
   ],
   "source": [
    "# Doc2Vec\n",
    "# gensim.models.word2vec\n",
    "# min_count = 500\n",
    "# size = 100\n",
    "# sg = CBOW\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "def get_data(subset):\n",
    "    newsgroups_data = fetch_20newsgroups(subset=subset, remove=('headers', 'footers', 'quotes'))\n",
    "    docs = []\n",
    "    for news_no, news in enumerate(newsgroups_data.data):       \n",
    "        tokens = gensim.utils.to_unicode(news).split() \n",
    "        if len(tokens) == 0:\n",
    "            continue\n",
    "        sentiment =  newsgroups_data.target[news_no]\n",
    "        tags = ['SENT_'+ str(news_no), str(sentiment)]\n",
    "        docs.append(TaggedDocument(tokens, tags))\n",
    "    print(docs)\n",
    "    \n",
    "    return docs\n",
    "train_docs = get_data('train')\n",
    "test_docs = get_data('test')\n",
    "alldocs = train_docs + test_docs\n",
    "\n",
    "model = Doc2Vec(size=size, window=window, alpha = alpha, negative=negative, sample=sample, min_count = min_count, workers=cores, iter=passes)\n",
    "model.build_vocab(alldocs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE or PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Document clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinhines/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 1:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 2:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 3:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 4:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 5:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 6:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 7:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 8:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 9:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 10:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 11:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 12:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 13:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 14:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 15:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 16:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 17:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 18:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n",
      "Topic 19:\n",
      "hello\n",
      "<generator object displaytopics.<locals>.<genexpr> at 0x1a160119e8>\n"
     ]
    }
   ],
   "source": [
    "# recycling bin block\n",
    "# all this code was moved down here to maybe use later\n",
    "# Dont run this block\n",
    "#HOLD (dont run for now)\n",
    "def displaytopics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\"hello\")\n",
    "        print(\" \".join([feature_names[i]])for i in topic.argsort()[:-no_top_words - 1:-1])\n",
    "        \n",
    "print(\"starting...\\n\\n\")\n",
    "nofeatures = 2000\n",
    "dataset = fetch_20newsgroups(remove=('headers','footers','quotes'))\n",
    "docs = dataset.data\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=nofeatures, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(docs)\n",
    "tf_featureNames = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 20\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online',learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "displaytopics(lda, tf_featureNames, no_top_words)\n",
    "\n",
    "\n",
    "\n",
    "# when we get the counts of each word, a larger document will have\n",
    "# a larger amount of words, which means the weight of larger\n",
    "# will be counted more than a smaller document\n",
    "# we use TF - Term Frequencies\n",
    "# Then , TF-IDF i.e Term Frequency times inverse document frequency\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#tfidf_transformer = TfidfTransformer()\n",
    "#X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "#X_train_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
